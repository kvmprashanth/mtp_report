\chapter{Memory management framework for derivative clouds}
  
  Our objective here is to provide a memory management framework for derivative clouds. 
  In order to achieve the same we start off with understanding existing memory management 
  and cache partitioning frameworks. We analyzed them to come up their drawbacks while 
  provisioning for a derivative cloud setup. We made updates to an previous caching partitioning
  framework to make it a more full pledged memory management framework than just a cache partitioning
  framework. My initial work involved understanding the existing infrastructure and come up with
  drawbacks and make updates to the design to accommodate the downfalls. We came up with a revised 
  design and implemented the same. We tested it for correctness and empirically evaluated it as well.
  
  \section{Drawbacks of existing framework}
    The following section tries to bring out the drawbacks of existing hypervisor cache partitioning 
    frameworks and how fail to satisfy application SLA requirements. We demonstrate how an two-level
    exclusive (either level-1 or level-2) cache partitioning framework could fail to satisfy requirements 
    and an intermediate partitioning framework would be desirable.
    
    \subsection{Experimental setup}
    \label{sec:dd_setup}
	
      The following section describes the experimental setup used to establish issues, verify correctness and evaluate our 
      solution. Any changes made to the below setup, shall be mentioned beforehand.
      
      \myparagraph{Testbed}	
	Our testbed consists of a single VM, single container running on top of our hybrid implementation of Double decker as shown 
	in Fig~\ref{img:correctness_testbed}. The hypervisor used is KVM, and the container manager used is LXC.  
	
	
	\noindent The physical machine configuration used is as described below,
	  \begin{enumerate}
	   \item Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz
	   \item 4 CPU cores (with multi-threading)
	   \item 8 GB of physical RAM
	   \item 120 GB SSD disk
	  \end{enumerate}

	
	\begin{figure}
	  \centering
	  \includegraphics[width=0.9\textwidth]{images/correctness/exp_setup.png}
	  \caption{Experimental testbed for checking correctness}
	  \label{img:correctness_testbed}
	\end{figure}
	
      \myparagraph{Experimental configurations}	
	The set of configurations used for an analysis of memory management framework for a derivative environment 
	must be relevant, and easy to apply. The following configurations
fit this criteria, and have been used for the evaluation.

	  \begin{itemize}
	   \item \textbf{Memory Requirement:} Memory requirement of each container, the estimated total memory used by a container.
	   
	   \item \textbf{Container memory limit:} Size of memory allocated to a container at the Cgroup level (soft and hard limits). 
	   \item \textbf{Memory cache limit:} Size of memory (L1) cache assigned to a container. 
	   \item \textbf{SSD cache limit:} Size of SSD (L2) cache assigned to a container.
	   
	   \item \textbf{Workload:} Workload application that is running inside each of the container. 
	   \item \textbf{Number of containers:} Number of containers that are currently executing in the system.
	   \item \textbf{Number of VMs:} Number of virtual machines that are currently executing in the system.
	  \end{itemize}	  
	  
	  For the sake of simplicity in the evaluations of correctness of our setup. We have only considered a single container, single VM setup 
	  which makes use of synthetic workload to stress our system.
	
      \myparagraph{Metrics of interest}	
	The following are the metrics of interest that would help us establish the correctness of our implementation.
	  
	  \begin{itemize}
	   
	   \item \textbf{Container memory usage:} Guest memory usage of the container.
	   \item \textbf{Memory cache usage:} Memory cache used by the container.
	   \item \textbf{SSD cache usage:} SSD cache used by the container.
	   
	   \item \textbf{Demoted:} Objects moved from memory to SSD cache.
	   \item \textbf{Promoted:} Objects moved from SSD to memory cache.
	   
	  \end{itemize}	  
	  
	  The following metrics are collected both for memory and SSD cache
	  
	  \begin{itemize}
	   
	   \item \textbf{Puts:} Number of objects successfully put into this container cache.
	   \item \textbf{Gets:} Number of objects successfully got from this container cache.
	   \item \textbf{Flushes:} Number of objects flushed from this container cache.
	   \item \textbf{Evicts:} Number of objects evicted from this container cache.
	   
	  \end{itemize}
  
    \subsection{Provisioning of caches at different levels based on application requirements}
      Previous results using \dd{}\cite{doubledecker} have shown that certain application requirements 
      (along with their configurations) could be satisfied better by provisioning on a cache of certain 
      performance guarantees. In the previous work\cite{doubledecker} it was shown how an application like 
      Mail-server could be better provisioned on the level-2 (SSD) cache, as its requirement involved 
      having a large WSS (working set size) with slow access rates. On contrary it also showed by cache sensitive 
      applications like the Web-server workload had to be provisioned onto the level-1 (memory) cache. 
      Now let's see if this setup of an exclusive two level cache provisioning schema would satisfy application
      specific need in all cases.
	
	\subsubsection{Inadequate exclusive two level cache provisioning}
	  Consider the following scenario to demonstrate the short comings of the existing exclusive two level cache
	  provisioning framework. Consider the case of a container with a WSS to provisioned onto the hypervisor cache with 
	  no-free memory to be further allocated at the guest VM and the hypervisor cache available is 0.5 GB at 
	  level-1 (Memory) and 1 GB at level-2 (SSD). Consider a Web-server workload where the desired application
	  throughput is 7000 op/s.
	  
	  \begin{figure}
	    \centering
	    \includegraphics[scale=0.8]{images/dd_hybrid_motivation/throughput.eps}
	    \caption{Inadequate exclusive two level cache provisioning framework}
	    \label{plot:dd_hybrid_motivation}
	  \end{figure}
	  
	  Using our existing double decker cache, we can at max provision at 6000 op/s using memory cache of 0.5 GB as 
	  shown in Fig~\ref{plot:dd_hybrid_motivation}. But say our requirement was beyond that at 7000 op/s. Is there a 
	  workaround to satisfy this requirement ? Could we come up with a better design to satisfy this configuration ?
	  
    
    \section{Inability of cache partitioning framework to support anonymous memory applications} 
	
	\begin{figure}
	  \centering
	  \includegraphics[scale=0.8]{images/dd_decentrailze_motivation/throughput.eps}
	  \caption{Split of memory allocation ratios (In-VM:Cache) to affect application performance}
	  \label{plot:dd_decentrailze_motivation}
	\end{figure}
	
	Application memory requirement is vastly classified into two types - \textit{anonymous and disk backed}.
	Application that require disk backed memory pages can be satisfied be either provisioning them at the 
	VM(guest) memory or at the hypervisor cache. This effect is seen in Fig~\ref{plot:dd_decentrailze_motivation}
	where the memory requirement is split in ratios of in-VM:cache allocations. The plot shows how applications like
	Web-server and \mongo{} which require disk backed pages can be satisfied either in-VM or at the hypervisor cache.
	Kindly note that the application performance is nearly constant in all cases of both these applications as the 
	in-VM memory and the hypervisor cache operate at similar speeds in this case, and had there been difference in
	operational speeds, we would have observed degradation in application performance when moving from left to right 
	in the plot.
	
	Now, if you look at applications like \redis{} and MySQL, their performance is highly affected while provisioning
	them on cache, as they heavily depend on anonymous memory allocated to them (especially Redis). Hence such applications
	need to be provisioned at the guest, however existing cache partitioning frameworks 
	\cite{koller2015centaur, schopp2006resizing} can provision for disk backed workloads fail to address the needs of 
	anonymous memory hungry workloads.
          

  \section{Rethink of existing design}
  
    \subsection{Decentralized memory management framework}
	      
      \subsubsection{Native provider cache partitioning framework}

      \subsubsection{Derivative provider memory management framework}	

    \subsection{Hybrid cache}
      
      \subsubsection{Multilevel configurable caches}
      
      \subsubsection{Movement of cache objects}
	
  \section{Implementation details}
      
    \subsection{Existing implementation status}
    
    \subsection{Hybrid cache}
    
      \subsubsection{Pools to accommodate both memory and SSD objects}
      
      \subsubsection{Asynchronous kernel threads for movement of objects}
      
      \subsubsection{Multilevel stats}
  
  \newpage
  
  \section{Correctness of implementation}
  
      The experimental setup is just as described in Section~\ref{sec:dd_setup}.
      For establishing the correctness of our workload, we have considered a self 
      generated synthetic workload generated using \texttt{cat} command that 
      outputs the content of a file onto \texttt{/dev/null}. This workloads helps 
      us to validate the correctness of our implementation by predicting deterministic
      outputs.
  
    \subsection{Arithematic validation of stats}
    
      \myparagraph{Question}
	    To verify the correctness in accounting of stats while accessing cache at both levels.
	    
      \myparagraph{Procedure}
	We ran several experiments and computed the actual cache usage (memory and SSD) in our implementation. To an calculated 
	an estimated cache usage we used the formula given below. We used the same formula for both memory and SSD cache. 
	  \begin{equation}
	    EstimatedUsed = Puts + ObjectsMovedIn - (Gets + Flushes + ObjectsMovedOut)
	  \end{equation}
      
      \myparagraph{Observations}
	The values for \textit{EstimatedUsed} and \textit{ActualUsed} (present in out stat counter) matched in most cases.
	However, over long periods of run, with quite a large number of cache operations there was a marginal difference
	between the two (\textless 1\%).
	
      \myparagraph{Inference}
	Since the correctness of the actual value of cache used depend on the other stats that we have collected, and the 
	matching of \textit{EstimatedUsed} and \textit{ActualUsed} would only mean that all the stats collected are right.
    
    \subsection{Movement of objects between both levels of cache}
	
	To verify the correctness of our implementation empirically, we have taken our synthetic workload described in 
	above and ran a couple of simple experiments to demonstrate the expected 
	behavior of our cache to support cache operations like puts, gets, promotions and demotions.  
	
	\subsubsection{Memory to SSD cache}

	  \myparagraph{Question}
	    To verify the correctness in accounting of stats while accessing cache and moving objects from memory (L1) to SSD (L2) cache.
	    
	  \myparagraph{Procedure}
	      We start of the experiment with powering on the VM, followed by the container. We assigned complete memory and SSD cache 
	    at the double-decker back-end to support the container. The container had a \textbf{memory requirement of 2078 MB}, 2048 MB workload
	    requirement and 30 MB container requirement (container requirement was obtained by running the same experiment while having
	    a nearly 0 MB workload. The container was allocated with 2048 MB (512 MB of container memory + 512 MB of memory cache 
	    + 1024 MB of SSD cache). Now, the workload performed a sequential read of its workload (i.e 2048 MB) once.
	    Table~\ref{table:correctness_memtossd} shows the list of approx. estimated values (which are based on our 
	    implementation) and observed values for the metrics at the cache at the end of the experiment. 
	    
	    \vspace*{1em}	
	      \begin{table}
		\begin{center}
		  \begin{tabular}{ r | p{4cm} | p{4cm} }	      	    
			Metric & Approx. estimated Value & Observed Value \\ 
		    \hline
		    \hline
		    Puts (MB) & 2078 & 2080 \\
		    \hline
		    Gets (MB) & 0 & 1 \\
		    \hline
		    Container memory usage (MB) & 512 & 509 \\  
		    \hline
		    Memory cache usage (MB) & 504 & 503 \\
		    \hline 
		    SSD cache usage (MB) & 1008 & 1000 \\
		    \hline
		    Evicts (MB) & 54  & 64 \\
		    \hline
		    Flushes (MB) & 0  & 0 \\
		    \hline
		    Cumulative usage (MB) & 2078 & 2076 \\
		    \hline
		    Demotions (MB) & 1062 & 1064 \\
		    \hline
		    
		  \end{tabular}
		\caption{Comparison between expected and actual values}
		\label{table:correctness_memtossd}
		\end{center}	  
	      \end{table}
	    \vspace*{1em}
	    
	  \myparagraph{Observations}
	  The following are the observations,	  
	    \begin{enumerate}
	     \item There is a small number of gets, probably occurring due to pages used by other container applications.
	     \item Puts in the cache, is marginally (\textless2 MB) greater than expected value, and this deviation is due to the small number 
	     of Gets which are occurring.
	     \item The memory cache usage, is exactly as expected. However, SSD cache usage is slightly lesser than the expected value, 
	     but however the deviation seems to be an acceptable value.
	     \item The demotions (movement from memory to SSD) and cumulative usage values are nearly the same with a subtle deviation 
	     (\textless2 MB) which is an acceptable value.
	    \end{enumerate}

	  
	  \myparagraph{Inference}
	    The accounting stats, are nearly as expected. This verifies the correctness of most of the stats of our implementation (except promotions).
	    
	
	\subsubsection{SSD to memory cache}
	  
	  \myparagraph{Question}
	    To verify the correctness in accounting of stats while moving objects from SSD (L2) to memory (L1) cache.
	    
	  \myparagraph{Procedure}
	      We start of the experiment with powering on the VM, followed by the container. We assigned complete memory and SSD cache 
	    at the double-decker back-end to support the container. The container had a \textbf{memory requirement of 2078 MB}, 2048 MB workload
	    requirement and 30 MB container requirement (container requirement was obtained by running the same experiment while having
	    a nearly 0 MB workload. The container was allocated with 2560 MB (512 MB of container memory + 2048 MB of SSD cache).
	    The workload performed a sequential read of its workload (i.e 2048 MB) once, this lead to using up of nearly 1566 MB of SSD cache.
	    
	    Now, we changed the memory cache size to 256 MB while performing basic operations at the container which triggered the promotion 
	    (movement of objects from SSD to memory) of the objects to the memory cache. The promotion triggers all objects until the memory 
	    cache reaches a threshold usage - 192 MB in our case, as this threshold is calculated as,
	    
	    \begin{equation}
	      MemoryCacheLowerThreshold (192 MB) = MemoryLimit (256 MB) - LimitSize (64 MB)
	    \end{equation}
	    
	    Hence we would expect 192 MB worth of objects be promoted from SSD to memory cache in an ideal case.
	    
	    
	  \myparagraph{Observation}
	    Using our stats, it was observed that the estimated promotion and the actual promotion of objects were of an exact match with 
	    the number being 192 MB.
	    
	  \myparagraph{Inference}
	    This verifies the correctness in the accounting stats in movement of objects from SSD to memory cache. Hence movement 
	    of objects from both memory to SSD and vice-versa have been verified empirically.
  
  
  
  \section{Evaluation of Double Decker}
  
    \subsection{Experimental setup}
	
	\paragraph{Experimental configurations}
	
	\paragraph{Metrics of interest}
	
	\paragraph{Workload}
	
	\paragraph{Testbed}
  
    %\subsection{Performance comparion with old implementation}
  
    \subsection{Provisioning for anonymous and file backed workloads}
    
    \subsection{Hybrid cache provisioning}
    