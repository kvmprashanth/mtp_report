
\chapter{Background}

  \section{Containers}
  
    Container in simple terms can be defined as, 
  
    \begin{center}
      \textit{``Container is a process or set of processes grouped together along \\ with its dependent resources into a single logical OS 
entity. \\It enables multiple isolated user-space instances on a host machine.''} 
    \end{center}
    
    Containers \cite{manual} are built as an extension to the existing operating system and not as an independent system. Container 
provides virtualization of isolated user spaces at an OS-level and hence containers executing on a host machine reuse the functionalities of 
the host kernel. This makes it better by reducing redudant kernel pages as used in VMs but comes at the cost of containers only of host OS 
type to execute on a system.

     \begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth]{images/vm_vs_container.jpg}
      \caption{A depiction of a derivative IaaS cloud platform, Source:\cite{slideshare}}
      \label{img_difference}
    \end{figure}

A high level difference between a VM and containers can be seen in Fig:\ref{img_difference}. The biggest advantage of using containers over 
virtual machines is that they provide much lesser performance overheads. Containers are usally managed by container managers, which are 
entities similar to how Hypervisors are to VMs. Container managers are shipped by different organizations like Docker \cite{docker}, LXD 
\cite{lxd}, OpenVZ \cite{kolyshkin2006virtualization} etc. All container managers makes use of 3 linux kernel components and combine 
them to form the building structure. The deploy their own controllers on top of this. 
    
    \begin{enumerate}
      \item Control Groups: Used for resource accounting and control
      \item Namespaces: Resource isolation among resources provisioned to different users on the same system
      \item Disk Images: The disk image which provides the ROOTFS for a container to execute. It contains the distribution related 
packages, libraries, and application programs.
    \end{enumerate}
    
    For the pupose of this discussion, we would focus on control cgroups (cgroups) as this provides the mechanism to control resources 
which includes performing memory management.

    
    \subsection{Control groups}
      
      \begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{images/cgroups.png}
	\caption{Control groups illustration using 3 controllers, Source:\cite{manual}}
	\label{img_cgroup}
      \end{figure}
      
      A solution to process group control and accounting was proposed by Google in 2007 which was originally called Generic Process 
Containers \cite{menage2007adding} and was later renamed to Control Groups (cgroups), to avoid confusion with the term Containers. A 
cgroup/subsystem refers to a resource controller for a certain type of CPU resource. Eg- Memory cgroup, Network cgroup etc. It is derives 
ideas and extends the process-tracking design used for cpusets system present in the Linux kernel. There are 12 different 
cgroups/subsystems, one for each resource type classified.

      For the purpose of our discussion we will stick to subsystem as the terminology referring to individual resource control and cgroup 
to refer a cgroup node in hierarchy. The Linux kernel by default enables most subsystems. The overheads introduced by cgroups are 
negligible. Most subsystems follow their own hierarchy for their individual resource. The Linux exposes Pseudo file systems as userspace 
APIs to interact with them.

      Fig:\ref{img_cgroup} illustrates a minimalistic outline of a cgroups hierarchy with 3 subsystems mounted in a system onto their own 
  hierarchies. The three subsystem mounted are - memory, cpuset and blkio and are mounted at \texttt{/sys/fs/cgroups/}. Memory root cgroup 
of   8GB is divided into two cgroups M1 and M2 of 4GB each. cpuset root cgroup of 4CPUs is divided into two cgroups C1 and C2 of 3CPUs and 
1CPU   respectively. blkio root cgroup of is divided into two cgroups B1 and B2 of 1000 and 500 as relative weights respectively. Every 
process   which attaches itself to the same set of subsystems are referred by a single \texttt{css\_set} which in turn points to the 
cgroup node the   process is attached to. In the Fig, processes 1,2 attach itself to the blue \texttt{css\_set} and 3,4,5 to the red one. 
The \texttt{css\_set} in turn has pointers to \texttt{container\_subsys\_state} that is one for each cgroup. Notice how the blue 
\texttt{css\_set} points to the   root cpuset cgroup there by assigning it all the CPUs in the system which is also a valid and 
default value to attach processes.
      
      \subsubsection{Memory subsystem}
	
	Memory subsystem use a common data structure and support library for tracking usage and imposing limits using the "resource 
counter". Resource controller is an existing Linux implementation for tracking resource usage. Memory cgroup subsystem allocates three 
\texttt{res\_counters}. The three of them are described below.

      \textbf{i. Accounting:} Accounting memory for each process group. Keeps track of pages used by each group. Pages can be classified 
into four types. 
      \begin{itemize}
	\item Anonymous: Stack, heap etc.
	\item Active: Recently used pages
	\item Inactive: Pages read for eviction
	\item File: Reads/Writes/mmap from block devices
      \end{itemize}
	
      \textbf{ii. Limits:} Limits can be set on each cgroups. Limits are of two types - soft and hard. Soft limit is the limit up to which 
the system guarantees availability. Hard limit is the limit up to which the system tries to accommodate, but cannot guaranty this if 
system is under memory pressure. Limits can be set in terms of byte for,
      \begin{itemize}
	\item Physical memory
	\item Kernel memory
	\item Total memory (Physical + Swap)
      \end{itemize}

      \textbf{iii. OOM:} Out Of Memory killers are used to kill processes or trigger any other such event on reaching hard limit by a 
process group. 

	More about memory management using memory subsystem in the Linux kernel shall be described in the coming section.
      
  \pagebreak
  
  \section{Memory Management between processes in Linux}
  
    Memory is allocated/deallocated in terms of pages in any operating system. Memory management in Linux is done using techniques like 
virtual memory, demand paging, swapping caching etc. They separate between the memory needed by a process and the memory physically 
allocated on the RAM. The OS creates a large virtual address space for each process. In this section we focus on how memory is managed 
between processes or a group of processes. We mainly focus on how memory is assigned and reclaimed between them.  
  
    \subsubsection{Memory Pages used by a process}
      Memory used by processes are divided into 2 types of pages
	
      \begin{enumerate}	
	\item Anonymous Pages: Pages those which are not associated with any files on disk. They are process memory pages.
	\item Page cache pages: Are an in-memory representation of a part files on the disks.	
      \end{enumerate}

  
    \subsubsection{Memory Allocation}
      When the process needs memory to be allocated, Linux decides the how this memory is going to be allocated physically on the RAM. The 
process/ application does not see in physical RAM addresses. It only sees virtual addresses from the virtual space assigned to each process.
The OS uses a page file located on the disk to assist with memory requests in addition to the RAM. Less RAM means more pressure on the Page 
file. When the OS tries to find a piece of memory that's not in the RAM, it will try to find in the page file, and in this case they call it 
a page miss. The actual physical memory allocated (RSS) to a process depends on how much free memory is available in the system. On free 
memory becoming freshly available in the system, the OS tries to equally distribute the available memory to processes that are demanding 
more memory.

    \subsubsection{Native Memory Reclamation}
      When the system memory starts to get tight, the kernel can free memory by cleaning up its own internal data structures - reducing the 
size of the inode and dentry caches however most pages in the system are user process pages. Hence the kernel, in order to accommodate 
current demands for user pages, must find some existing pages to toss out. A proper balance between anonymous and page cache pages must be 
maintained for the system to perform well. Kernel offers a knob called swappiness, that specifies how much favor anonymous versus page 
cache pages while reclamation. The default value for swappiness favors the eviction of page cache pages. 
      
      The system maintains two LRU lists commonly referred as LRU/2, one active list containing all the pages that were recently used and 
another inactive list which contains all the pages that weren't used recently. One pair (active and inactive) for anonymous pages and one 
pair for page cache pages. The kernel favors reclamation from page cache pages over anonymous pages and inactive pages over active pages 
and iterates these lists to satisfy reclamation requests there by trying to maximize application performance while satisfying requests.
    
    \subsubsection{Memory Reclamation with Container support}
      
      \begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{images/page_mapping.jpg}
	\caption{Mapping of pages to LRU lists}
	\label{img_page_mapping}
      \end{figure}
    
      The following section describes the existing policy. The current system-wide policy incorporates memory reclamation keeping in mind 
the memory cgroups. Reclamation can broadly occur in two situations,
      \begin{enumerate}
	\item \textbf{System-Wide (Global) Reclamation:} When the system is under memory pressure when all/most of its pages are occupied
	\item \textbf{Container Specific (Local) Reclamation:} When only a particular of the container is under pressure due to exceeding 
its hard limit
      \end{enumerate}
      
      For the purposes of our problem, we focus on System-Wide Reclamation. It must be remembered that System-wide reclamation can again 
broadly occur in two situations,       
      \begin{enumerate}
	\item \textbf{Synchronous:} When system is under memory pressure due to new page requests and not enough free pages available
	\item \textbf{Asynchronous:} System clears up memory routinely when free
      \end{enumerate}         
      
      Both Synchronous and Asynchronous global reclamation ultimately end up taking a similar path for memory reclamation, with little 
differences when it comes to regions to reclaim from and how much to reclaim. A kernel function call trace for system-wide reclamation is 
described in Fig:\ref{img_call_trace}. It shows how sync and async requests are ultimately mapped to the same set of function calls.

      \begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{images/reclamation_call_trace.jpg}
	\caption{Kernel Function Call Trace for System-Wide Reclamation}
	\label{img_call_trace}
      \end{figure}

      \begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{images/High_level_Reclamation.png}
	\caption{Existing policy for Memory Reclamation}
	\label{img_high_level}
      \end{figure}  
      
      As shown in Fig:\ref{img_page_mapping}, in recent linux kernels a LRU (LRU/2, but LRU used for simplicity) list is stored for every 
container created also there is a Global LRU list which contains the pages of all processes in the system (including in the ones in a new 
container). All processes are by default put into the default container and hence its pages are a part of the global LRU.  Once a process is 
moved to a specific container, its pages also become a part of its local per container LRU list also.
    
      An container (Memory cgroup) when has its soft limit set, has a value called excess computed for it at every container node. The 
system internally makes use of a RBTree to store the exceeds. The exceed is computed at \texttt{mem\_cgroup\_update\_tree()} at regular 
intervals using the formula,
      
      \begin{center}
	$ excess = usage - softlimit $
      \end{center}
      
      \begin{itemize}
       \item \textbf{Soft Memory Reclamation (SMR):} is the one specific to cgroups where container which exceeds its 
soft limits by the most is reclaimed from using the per container based LRU. It internally also reclaims from all its child containers.  
       \item \textbf{Global LRU reclamation policy (GLR):} reclaims based on the global LRU list in which all processes in the system are a 
part of
      \end{itemize}     
      
      The existing reclamation policy is illustrated as a flow chart in Fig:\ref{img_high_level}. In simple terms the global reclamation 
policy is a combination of both Soft Memory Reclamation and the native global LRU based reclamation. The reclamation algorithm tries to 
maximize most of the requested reclamation from SMR and prefers GLR only when requests aren't being satisfied by SMR.
