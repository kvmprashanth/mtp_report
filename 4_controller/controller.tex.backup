
\chapter{Differentiated memory management controller for containers}

  Overview here. TBD
  
  \section{Drawbacks of existing memory management for containers}
  \label{sec:controller_issues}
  
    TBD.
    
    \subsection{Experimental setup}
    
      The following are the experimental setup configurations, metrics and workloads used to establish the correctness of our hypotheses
      in a native cloud environment.
      
       \subsubsection{Experimental configurations}
    
	The set of configurations used for an analysis of memory management techniques in a container environment must be relevant, 
	and easy to apply. The following configurations fit this criteria, and have been used for the evaluation.
	
	\begin{itemize}
	  \item \textbf{Number of containers:} The number of containers that are currently executing in the system.
	  \item \textbf{Memory soft limit of container:} The minimum promised memory to a given container by the system on which the 
    container is executing.
	  \item \textbf{Memory hard limit of container:} The maximum memory that can be assigned to a container by the system on which the 
    container is executing.
	  \item \textbf{Memory usage of each container:} The usage of a container at a given point in time, that is generated by the workload 
    executing inside the container.
	  \item \textbf{Workload:} The workload that is running inside each of the container. Workloads can vary based on the type of operation 
    they perform, the ratio of anonymous memory pages they consume to that of page cache pages.  
	  \item \textbf{External memory pressure:} The memory pressure that is generated in the system in order to reduce the free memory 
    available in the system and trigger memory reclamation. This pressure could be either generated by a process on the same system / driver 
    that is running in the host system.
	  \item \textbf{Size of machine:} Size of Machine refers to the maximum memory available in the system inside which all the 
    containers are executing.      
	\end{itemize}  
      
      \subsubsection{Metrics of interest}
      
	The following are the metrics of interest to us that would help us analyze the experiments.
	  
	    \begin{enumerate}
	    \item \textbf{Memory assigned to each container:} Total memory assigned to a container at any given instant
	    \item \textbf{Soft memory reclaimed for each container:} Memory reclaimed from each container using SMR
	    \item \textbf{Total memory reclaimed for each container:} Total Memory reclaimed from container (SMR + GLR)
	    \item \textbf{Memory reclaimed using GLR:} Memory reclaimed from all containers and other processes running on system using GLR
	    \item \textbf{Memory reassigned for each container:} Memory reassigned to each container on freeing up of memory
	    \item \textbf{Application specific metrics:} Application metrics of the workload running inside containers like throughput, total 
    time taken etc.
	    \end{enumerate}
      
      \subsubsection{Workloads}
      
	This section presents the list of workloads that we have used as primary candidates to evaluate our empirical evaluations. All 
    workloads are chosen keeping in mind the memory intensive nature of the requirement.  
	
	These are the list of Synthetic workloads we have used to establish our problem.
	  
	  \myparagraph{Stress}
	    Stress \cite{stress} is a deliberately simple workload generator for POSIX systems. It imposes a configurable amount of CPU, 
    memory, I/O, and disk stress on the system. It is written in C and has been developed by people at Harvard university. 
	  
	  \myparagraph{Memory hogger}
	    Memory Hogger is a simple C program that allocates an array of specified memory using a simple \texttt{malloc()} and repeatedly 
    writes to these array locations. This only consumes anonymous memory pages.
	  
	  \myparagraph{File hogger}
	    File Hogger is a simple python program that creates a file with specified size and repeatedly updates it line by line there by 
    consuming both anonymous pages and file backed pages.

	These are the list of real workloads we have used to show how the existing problems affect real work applications.
	  
	  \myparagraph{MongoDB}
	    MongoDB \cite{Mongodb} is an open-source, document database designed for ease of development and scaling. Classified as a NoSQL 
    database program, MongoDB avoids the traditional table-based relational database structure in favor of JSON-like documents with dynamic 
    schema. It follows a memory hungry approach where it tries to use up most of system and it actually leaves it up to the OS's VMM to tell it 
    to release the memory.

	\myparagraph{Redis}
	    Redis \cite{Redis} is a in-memory data structure store, used as database, cache and message broker. It is used to store a large 
    number of in-memory key-value pairs. Its in-memory nature makes it a prime candidate to use it as a workload in our empirical evaluations.
      
	\myparagraph{YCSB benchmark}
	    We use YCSB \cite{cooper2010benchmarking} (Yahoo Cloud Server Benchmark) project as the benchmark to generate the clients evaluate 
    to the performance of our real workloads i.e MongoDB and Redis servers. The goal of YSCB is to develop performance comparisons of the new 
    generation of cloud data serving systems. It is a framework and common set of workloads for evaluating the performance of different 
    “key-value” stores.
    
  
    \subsection{Issues in native environment}
    
      We have tried to show the issues in container memory management using empirical analysis. We have taken our hypothesis/questions, 
      and mapped them to various experiments to illustrate the issues. Inferences were drawn based on the observations in the experiments. 
    
      \begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{images/controller_issues/native_testbed.png}
	\caption{Native container testbed}
	\label{img:native_setup}
      \end{figure}
      
       The native testbed consisted of running containers inside a host machine (running inside VM in our case) in complete isolation from 
the external environment as shown in Fig~\ref{img:derived_setup}. This setup which involved a native container testbed, was used to 
understand the existing memory reclamations and establish the problem in a native system using \textbf{synthetic workloads}.
	
      \subsubsection{Host}
	
	\begin{enumerate}
	  \item Intel Core i5-4430 processor @ 3.00GHz
	  \item 4 cores of CPU (with hyper threading support)
	  \item 1 TB of hard disk space
	  \item 8 GB RAM
	  \item Ubuntu 14.04 LTS desktop, 64 bit 
	  \item Kernel version 4.5
	  \item KVM Hypervisor
	\end{enumerate}
      
      \subsubsection{Guest}
	
	\begin{enumerate}
	  \item 3 cores of CPU (with hyper threading support)
	  \item 20 GB of virtual disk space
	  \item 2-6 GB RAM (based on experimental configuration)
	  \item Ubuntu 16.04 LTS desktop, 64 bit
	  \item Kernel version 4.7
	  \item Container technology: Docker
	\end{enumerate}
	
      Memory Hogger and File Hogger were used to generate the memory pressure inside the containers. External pressure was generated 
using Stress workload running directly on the host machine.
  
  
      The configuration in Table~\ref{table_native_base} is the base configuration for all experiments in this section. Any changes the base 
      configuration has been mentioned in the procedure of each of the experiment.

	\begin{table}	 
	  \begin{center}
	    \begin{tabular}{ l | c | c }
	      & Container-1 (M1) & Container-2 (M2) \\ 
	      \hline
	      \hline
	      Size of VM & \multicolumn{2}{c}{2 GB} \\	      
	      \hline
	      Workload & Memory Hogger & Memory Hogger \\
	      \hline
	      Hard Limit & 1000 MB & 1000 MB \\  
	      \hline
	      Soft Limit & 150 MB & 150 MB \\  
	      \hline
	      Memory Usage & 500 MB & 500 MB \\
	      \hline
	      Exceed & 350 MB & 350 MB \\
	      \hline 
	      External Pressure & \multicolumn{2}{l}{ 200 - 400 - 600 - 800 - 1000 MB} \\
	    \end{tabular}	    
	    \caption{Base configuration for native container experimentation}
	    \label{table_native_base}
	  \end{center}
	\end{table}
	
      Most experiments involved setting up of 2 containers. Workloads were used to introduce system memory pressure from containers. At 
      this point there was no memory pressure in the system (free memory was still available). Now the external pressure using Stress was 
      introduced after about 20s which created memory pressure in the system that triggered reclamation. The external pressure kept on increasing 
      by 200 MB in intervals of 40s. Each interval had a gap of 10s for memory to be reassigned to containers.
      
      \subsubsection{Reclamation above soft limits}
      
	\myparagraph{Hypothesis} 
	  Hypothesis to be verified,
	  \begin{enumerate}
	    \item Majority of reclamation when containers exceed occurs using SMR (Soft Memory Reclamation) 
	    \item SMR purely based on exceed value of the container
	    \item Containers that exceed equally are iteratively targeted
	  \end{enumerate} 
	  
	  \myparagraph{Procedure}
	    To demonstrate the correctness of our hypothesis we the base configuration described in Table~\ref{table_native_base} and change 
  the usage to 700 MB and soft limit to 350 MB there by simulating an scenario (Exp-1) where \textbf{Both containers exceeded by the same 
  values}.
	  
	  \begin{figure*}[t!]
	    \centering
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/exceed_only/Exceed.png}
	      \caption{Memory exceed plot}
	      \label{img_exceed_only_1_exceed}
	    \end{subfigure}
	    ~ 
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/exceed_only/Memory_Reclaimed.png}
	      \caption{Cumulative soft memory reclaimed plot}
	      \label{img_exceed_only_1_smr}
	    \end{subfigure}
	    ~ 
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/exceed_only/compare.png}
	      \caption{SMR versus GLR}
	    \label{img_exceed_only_1_compare}
	    \end{subfigure}
	    \caption{Plots for analysis of reclamation when both containers are exceeding by same value}
	  \end{figure*}
	  
	  \myparagraph{Observations}
	    The following are the observations,
	    \begin{itemize}
	      \item As seen from Fig~\ref{img_exceed_only_1_exceed}, Fig~\ref{img_exceed_only_1_smr} - memory reclaimed from containers 
  iteratively from one after the other as their exceeds are same.
	      \item Fig~\ref{img_exceed_only_1_compare} shows how most reclamation when containers exceed occurs using SMR however it is seen 
  that there is minimum reclamation occurring using GLR as well.
	    \end{itemize}

	  \myparagraph{Inference}	
	    The following are the inferences,
	    \begin{itemize}
	      \item SMR is purely based on exceed value.
	      \item Most reclamation when containers exceed occurs using SMR, however the GLR kicks in every reclamation request to evict any 
  inactive page cache pages in the system (may/may not belong to container).
	      \item Containers that exceed equally are iteratively target for reclamation one after the other.
	    \end{itemize}
      
      \subsubsection{Reclamation below soft limits}
      
      \myparagraph{Hypothesis}
	Does our hypotheses of reclamation below soft limits falling back to native system reclamation hold good ?
	
      \myparagraph{Procedure}  
	To test the reclamation patterns in containers below soft limits, we created containers as mentioned in Table~\ref{table_native_base} 
  and changed soft limits (Exp-2) of both containers to 1000 MB there by making the current \textbf{usage of both containers below soft 
  limits}. We used hooks in the kernel code to track requests satisfied by soft memory reclamation (SMR) and global LRU based reclamation 
  (GLR).
	
	\begin{figure*}[t!]
	    \centering
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/global_lru/mu.png}
	      \caption{Memory Usage Plot}
	      \label{img_no_sl_mu}
	    \end{subfigure}
	    ~ 
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/global_lru/compare.png}
	      \caption{SMR versus GLR Plot}
	      \label{img_no_sl_global_vs_local}
	    \end{subfigure}
	    \caption{Plots for when both containers are having same usage but no exceeds}
	  \end{figure*}
	
	\myparagraph{Observations}
	  The following are the observations.
	  \begin{itemize}
	    \item As seen from Fig~\ref{img_no_sl_mu}, there is no hand-in-hand reclamation that occurs to containers below their soft 
  limits although the containers are running the same workload, unlike hand in hand reclamation that occurs in memory usage above soft limits.
	    \item Since both containers are below SL, all reclamation is occurring using the GLR (Global LRU based reclamation) as seen by 
  Fig~\ref{img_no_sl_global_vs_local}
	  \end{itemize}

	\myparagraph{Inferences}
	  The following are the inferences.
	  \begin{itemize}
	    \item Containers with memory usage below soft limits reclamation falls back to native system GLR.
	    \item Reclamation using GLR is haphazard and there is no control over it. 
	  \end{itemize}
   
     \subsubsection{Effect of workloads characteristics on reclamation}
      
	\myparagraph{Question}
	Questions of our interest,
	    \begin{enumerate}
	      \item Effect of workload characteristics on reclamation
	      \item How much of memory is reclaimed from a container in a single reclamation SMR request ?
	    \end{enumerate}
	  
	  \myparagraph{Procedure}
	    We took our base configuration as described in Table~\ref{table_native_base}. However we ran two workloads in this case - Memory 
  Hogger (Exp-4a) and File Hogger (Exp-4b) workloads on it as native theory suggests that containers with page cache pages might be 
  victimized at larger the way it occurs with GLR.	
	  

	  \begin{figure*}[t!]
	    \centering
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/workload/1/Exceed.png}
	      \caption{Exceed plot for Experiment-4a}
	      \label{img:workload_1_exceed}
	    \end{subfigure}
	    ~ 
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/workload/2/Exceed.png}
	      \caption{Exceed plot for Experiment-4b}
	      \label{img:workload_2_exceed}
	    \end{subfigure}
	    \caption{Plots for analyzing effect of workloads characteristics on reclamation}
	  \end{figure*}		
	    
	  \myparagraph{Observations}
	  The following were the observations,	  
	  \begin{itemize}
	    \item The exceed goes hand in hand as expected but with larger deviation in Fig~\ref{img:workload_1_exceed} and 
  Fig~\ref{img:workload_2_exceed}
	    \item The larger deviation can be accounted to larger reclamation chunks in workloads that have page cache pages similar to how 
  reclamation targets page cache pages in native system
	    \item Further empirical analysis of the reclamation chucks gave us the reclamation chucks to be 
		\begin{center}
		    Reclamation chuck = Anonymous memory pages (\textless 25MB) + Page cache pages
		\end{center}
		In both cases pages from inactive zones were reclaimed before trying to reclaim from active lists.
	  \end{itemize}

	  \myparagraph{Inference}
	    Workloads with page cache pages are reclaimed at larger chunks per SMR request
	  
      \subsubsection{Key Implications}
    
	Here are the list of key implications that were derivative from running the above experiments in an synthetic environment. We have 
    classified it based on the scenarios as discussed earlier.
	
	\begin{enumerate}
	  \item When containers usage are above soft limits most reclamation occurs using SMR, however the GLR kicks in every reclamation 
    request to evict any  inactive page cache pages in the system (may/may not belong to container).      
	  \item SMR is purely based on exceed value of a container.
	  \item Workloads with page cache pages are reclaimed at larger chunks per SMR request
	  \item Containers with memory usage below soft limits reclamation falls back to native system GLR.
	  \item Reclamation using GLR is haphazard and there is no control over it. 
	\end{enumerate}
	
  
    \subsection{Amplification of issue in derivative clouds}
    
      The following experiment tries to establish the implications of previously established inferences, as to how these affect 
applications running on a derived cloud environment.

      \subsubsection{Testbed}

      The derivative cloud testbed consisted of running server containers inside a virtual machine (VM-1) which was running on top of a 
physical host machine. Another virtual machine (VM-2) was used to generate clients who connected to servers containers running inside VM-1 
as shown in Fig~\ref{img:derived_setup}. This setup was used to understand the impact of existing memory reclamation patterns on real 
workloads running on a derivative cloud setting.
      
      \begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{images/controller_issues/derivative_setup.png}
	\caption{Derivative cloud testbed}
	\label{img:derived_setup}
      \end{figure}
      
      \paragraph{Host}	
	\begin{enumerate}
	  \item Intel Xeon E5507 @ 2.27GHz
	  \item 8 cores of CPU (with hyper-threading support)
	  \item 125 GB of attached storage, Unlimited NFS attached storage 
	  \item 24 GB RAM
	  \item Ubuntu 14.04 LTS server, 64 bit 
	  \item Kernel version 3.13
	  \item KVM Hypervisor with memory ballooning enabled
	  \item Guest machines were connected using a software bridge
	\end{enumerate}
      
      \myparagraph{Guest}
	The two VMs used in this setup are described here.
	
	\noindent \textbf{VM-1:} Running server containers
	\begin{enumerate}
	  \item 6 cores of pinned CPUs (with hyper threading support)
	  \item 175 GB of virtual disk space (Storage was provisioned using NFS)
	  \item 16 GB RAM
	  \item Ubuntu 16.04 LTS desktop, 64 bit
	  \item Kernel version 4.7
	  \item Container technology: Docker
	  \item Containers inside guest were multiplexed using NAT forwarding
	\end{enumerate}
	
	\noindent \textbf{VM-2:} Running clients that connect to server containers
	\begin{enumerate}
	  \item 1 core of pinned CPU (with hyper threading support)
	  \item 20 GB of virtual disk space (Storage was provisioned using NFS)
	  \item 6 GB RAM
	  \item Ubuntu 16.04 LTS desktop, 64 bit
	  \item Kernel version 4.7
	\end{enumerate}
	
	Redis and MongoDB was used to generate the memory pressure inside the containers. External pressure was generated by varying guest 
balloon size triggered from the host.

      
    \subsubsection{Experimental Flow}
      
      To establish the limitations of nesting-agnostic memory reclamation we performed
      the following experiments. Two Linux+KVM virtual machines were used, one in a nesting setup,
      which hosted containers, the second generated workloads for the 
      container-hosted applications. The nested VM was provisioning with 6 vCPUs
      and 16 GB memory and the workload generating VM has allocated 1 vcPU and 6 GB memory.
      The nesting setup consisted of four Docker containers~\cite{docker}
      which executed with \redis{}~\cite{Redis} and
      \mongodb{}~\cite{mongodb} workloads. 
      Default configurations of the applications executed within
      the contains is as shown in Table~\ref{tbl:table_default_config}.
      %The four containers consisted of 2 containers running Redis \cite{redis} and MongoDB \cite{mongodb} workloads each. 
      The YSCB~\cite{cooper} workbench was used to generate workload datasets and 
      as a workload generator. For the first 100 seconds the applications were executed without 
      memory pressure to consume as much memory as required. Beyond 100 seconds, memory pressure
      was generated from the host and memory from the nested VM reclaimed at rate of 2 GB
      every 30 seconds.

      \begin{table}[t]
	  \begin{center}	   
	    \begin{tabular}{| l | c | c | c | c |}
		  \hline
	                &   & \sol  & Key Size  & Usage  \\ 
	      Container & (GB) & (GB)  & (\# records) & (GB) \\ 
	      \hline
	      \hline
	      Redis-Low & 2 & 0.5 & 500K & 1.3 \\  
	      \hline
	      Redis-High & 4 & 1 & 1000K & 2.6 \\  
	      \hline
	      Mongo-Low & 2 & 0.5 & 500K & 1.3 \\
	      \hline
	      Mongo-High & 4 & 1 & 1000K & 2.6 \\
	      \hline
	    \end{tabular}	  
	  \end{center}
	  \caption{Default configuration of the four application containers running in the derivative cloud setup}
	  \label{tbl:table_default_config}	  
	\end{table}
    
     \subsubsection{Impact in derivative environment}
	
      \begin{figure*}
	      \begin{subfigure}{0.33\textwidth}
      %	  \includegraphics[scale=0.44]{images/inference/memory_usage_mongo.png}
		\includegraphics[width=\textwidth]{images/controller_issues/derivative_issues/memory_usage_redis.png}
		\caption{\footnotesize Memory usage for \redis{} applications.}
		\label{plot_inference_redis}
	      \end{subfigure}
	      \begin{subfigure}{0.33\textwidth}
		\includegraphics[width=\textwidth]{images/controller_issues/derivative_issues/exceed_redis.png}
		\caption{\footnotesize Extent of memory usage exceed for \redis{} applications.}
		\label{plot_exceed_redis}
	      \end{subfigure}
	      \begin{subfigure}{0.33\textwidth}
		\includegraphics[width=\textwidth]{images/controller_issues/derivative_issues/memory_ratio.png}
		\caption{\footnotesize Memory allocation ratios.}
		\label{plot_inference_ratio}
	      \end{subfigure}
      \caption{Plots illustrating limitations in maintaining memory ratios in derivative clouds}
      \end{figure*}
    
    
  \section{Requirements for a new memory management controller}
  
    We wish to build an updated memory management controller that is controller aware, and is able to enforce a differential management
    policy by a native or derivative cloud provider. The following are the list of requirements of policies that we would like to enforce
    using this controller,
  
    \begin{enumerate}
      \item \textbf{Prioritized memory allocation:} Currently the notion of priority doesn't exist in container specific memory allocation 
although the notion of priority exists in other resources. The existing knobs fail to enforce priority used to manage memory in containers. 
      \item \textbf{Deterministic provisioning:} The policy to be designed must eliminate existing non determinism that exists while 
managing memory between containers in existing system.
      \item \textbf{Elastic provisioning:} Memory allocated must be resizable as and when required. 
      \item \textbf{Adaptive:} On changing resources provisioned to the system as in the case of an derivative environment, the policy 
enforced must still do it's best in maintaining promised QOS.
      \item \textbf{Differentiated memory reclamation:} The policy could build around the notion of differentiated memory reclamation when 
the system falls under memory pressure.
      \item \textbf{Strict enforcement of limits:} The notion of hard and soft limits that exist must be strengthened.      
    \end{enumerate}
    
  We would like to design a new controller keeping the above requirements in mind.
   
  
  \section{Proposed memory management controller}
      
      Enforcing memory allocation proportions across containers when no memory pressure exists is possible with the soft-limit and hard-limit 
      configuration parameters. However, as discussed in Section~\ref{sec:controller_issues}, these knobs do not provide deterministic memory 
      provisioning when the system is under memory pressure. As part of this work, we design for two new policies to provide deterministic 
      memory provisioning in nested setups under memory pressure
      
      \subsection{Controller logic}
	When the VM in which the containers are executing comes under memory pressure, memory is reclaimed using the container specific LRU lists 
	(as SMR) as well as the Global LRU list (as GLR) by the guest operating system.
	Memory reclamation depends on the extent of \textit{exceed} in memory usage above the specified \sol---estimated as the difference 
	in the two values. 
	The container having a higher \textit{exceed} is victimized during memory pressure first.
	We redefine this notion of \textit{exceed} with one that is based on proportionality weight of each container. 
	The proportional allocation of a container is the ratio of the container weight to the total weight across all containers in a 
	VM multiplied by the total memory usage across all containers (Equation~\ref{eqn:pa}). The proportionate \textit{exceed} value is the 
	difference of the container's proportional memory allocation and its current usage (Equation~\ref{eqn:ex}). 
	Finally, the memory reclamation policy is modified to target the container having  the highest proportionate \textit{exceed} value for each
	reclamation request. Similar to the default reclamation policy, the global LRU list for reclamation is used once memory usage 
	of all containers is below \sol{} specifications.
	
	\small
	\begin{align}
	  T\_U &= \sum_{i=1}^{n}U_{i} \\
	  T\_W &= \sum_{i=1}^{n}W_{i} \\
	  PA_{i} &= T\_U \times (\frac{W_{i}}{T\_W}) \label{eqn:pa}\\
	  EX_{i} &= U_{i} - PA_{i} \label{eqn:ex}
	\end{align}

	\footnotesize
	\textit{
	\\
	U$_{i}$: Memory usage of i$^{th}$ container \\
	W$_{i}$: Relative weight of i$^{th}$ container \\
	T\_U: Total memory usage by all containers \\
	T\_W: Summation of relative weights of all containers \\
	PA$_{i}$: Proportional memory allocation of i$^{th}$  container \\
	EX$_{i}$: Proportionate exceed value of i$^{th}$ container
	}
	\normalsize
    
      \subsection{Policies supported by our controller}
      
	The following are the policies enforceable by our controller.  
	
	\subsubsection{Policy 1: Proportionate memory allocation}
	  This policy aims to ensure that all memory allocated to container is based on relative weights specified as configuration parameters.
	  The proportional memory allocation is enforced during situations of memory pressure and during no pressure. For example, consider 
	  two containers with an intended proportionate memory allocation in the ratio 1:2. Further, assume that their \sol{} values are set 
	  to 1 GB and 2 GB, respectively, and their current usage is 2 GB and 4 GB respectively.
	  With a memory reclamation demand of 1 GB, most of the memory will be reclaimed from the container with the larger extent of usage, 
	  since the extent of exceed is 2 GB as compared to 1 GB of the smaller container. The resulting usages after reclamation will be 2 GB 
	  and 3 GB, respectively, violating the proportionate ratio of 1:2. The proposed proportionate allocation policy aims to maintain 
	  the 1:2 ratio in all memory pressure situations.
	
	\subsubsection{Policy 2: Application-specific differentiated allocation}
	  This policy provides the flexibility of providing specific rules and or categories to application containers, e.g. gold, silver,
	  bronze etc. Containers mapped to each of these categories have different reclamation rules or the categories themselves can 
	  imply an ordering for reclamation.
  
  \section{Modifications made to Linux memory Cgroup}
      
      We modify memory Linux \cg{} memory management subsystem to conform to the polices specified earlier. We have made
      three major changes to this subsystem as described below.
     
    
    \subsection{Per container configurable weights}
      A new per-cgroup state variable is introduced to specify a \emph{weight}
      parameter for each container and is also
      exposed through \texttt{sysfs} interface for every container.
      These weights enforce a notion of priority among containers; 
      higher the weight for a container the higher its priority and proportion.
      The relative ratio of the weights, dictate the proportion of memory allocation.
      Cloud providers can modify these weights in the derivative setup 
      (also applicable to native cloud providers using 
      containers for provisioning) to enforce priority among containers 
      executing within VMs.
    
    \subsection{Flexible reclamation size}
      The current soft-memory reclamation policy (SMR), targets one container and 
      reclaims a non-deterministic amount of memory from it for every 
      request. The extent of reclamation depends on the anonymous region (from which
      a fixed size is reclaimed) and the disk page cache (from which a large
      portion is reclaimed). Since the page cache size is non-deterministic,
      reclamation extent can also be non-deterministic.
      This may lead to a container being targeted exclusively for a particular 
      reclamation request, leading to large deviations 
      from weighted allocations. To overcome this issue, we have 
      capped reclamation chunk size to a maximum of 50 MB. 
    
    \subsection{Deterministic reclamation}
      As part of the Linux \cg{} memory reclamation process, soft-reclamation (SMR)
      reclaims memory from each container till memory usage is above the
      \sol specification. Simultaneously, a small amount of memory is also
      reclaimed from the system-wide pool of pages (GLR). In our solution,
      to maximize deterministic reclamation we perform proportionate
      reclamation across containers till the usage of containers reduces
      to zero. Beyond this situation, the system-wide global LRU list is
      used for further reclamations.
      Since the SMR policy is container-aware, this strategy
      provides us a better handle over memory provisioning for containers. 
    
  
  \section{Empirical evaluation of our controller}
  
    \subsection{Effectiveness of our controller}
    
    \subsection{Differential QOS containers}
    
    \subsection{Impact of reclamation chunk size}