
\chapter{Differentiated memory management controller for containers}

  Overview here. TBD
  
  \section{Drawbacks of existing memory management for containers}
  
    TBD.
    
    \subsection{Experimental setup}
    
      The following are the experimental setup configurations, metrics and workloads used to establish the correctness of our hypotheses
      in a native cloud environment.
      
       \subsubsection{Experimental configurations}
    
	The set of configurations used for an analysis of memory management techniques in a container environment must be relevant, 
	and easy to apply. The following configurations fit this criteria, and have been used for the evaluation.
	
	\begin{itemize}
	  \item \textbf{Number of containers:} The number of containers that are currently executing in the system.
	  \item \textbf{Memory soft limit of container:} The minimum promised memory to a given container by the system on which the 
    container is executing.
	  \item \textbf{Memory hard limit of container:} The maximum memory that can be assigned to a container by the system on which the 
    container is executing.
	  \item \textbf{Memory usage of each container:} The usage of a container at a given point in time, that is generated by the workload 
    executing inside the container.
	  \item \textbf{Workload:} The workload that is running inside each of the container. Workloads can vary based on the type of operation 
    they perform, the ratio of anonymous memory pages they consume to that of page cache pages.  
	  \item \textbf{External memory pressure:} The memory pressure that is generated in the system in order to reduce the free memory 
    available in the system and trigger memory reclamation. This pressure could be either generated by a process on the same system / driver 
    that is running in the host system.
	  \item \textbf{Size of machine:} Size of Machine refers to the maximum memory available in the system inside which all the 
    containers are executing.      
	\end{itemize}  
      
      \subsubsection{Metrics of interest}
      
	The following are the metrics of interest to us that would help us analyze the experiments.
	  
	    \begin{enumerate}
	    \item \textbf{Memory assigned to each container:} Total memory assigned to a container at any given instant
	    \item \textbf{Soft memory reclaimed for each container:} Memory reclaimed from each container using SMR
	    \item \textbf{Total memory reclaimed for each container:} Total Memory reclaimed from container (SMR + GLR)
	    \item \textbf{Memory reclaimed using GLR:} Memory reclaimed from all containers and other processes running on system using GLR
	    \item \textbf{Memory reassigned for each container:} Memory reassigned to each container on freeing up of memory
	    \item \textbf{Application specific metrics:} Application metrics of the workload running inside containers like throughput, total 
    time taken etc.
	    \end{enumerate}
      
      \subsubsection{Workloads}
      
	This section presents the list of workloads that we have used as primary candidates to evaluate our empirical evaluations. All 
    workloads are chosen keeping in mind the memory intensive nature of the requirement.  
	
	These are the list of Synthetic workloads we have used to establish our problem.
	  
	  \myparagraph{Stress}
	    Stress \cite{stress} is a deliberately simple workload generator for POSIX systems. It imposes a configurable amount of CPU, 
    memory, I/O, and disk stress on the system. It is written in C and has been developed by people at Harvard university. 
	  
	  \myparagraph{Memory hogger}
	    Memory Hogger is a simple C program that allocates an array of specified memory using a simple \texttt{malloc()} and repeatedly 
    writes to these array locations. This only consumes anonymous memory pages.
	  
	  \myparagraph{File hogger}
	    File Hogger is a simple python program that creates a file with specified size and repeatedly updates it line by line there by 
    consuming both anonymous pages and file backed pages.

	These are the list of real workloads we have used to show how the existing problems affect real work applications.
	  
	  \myparagraph{MongoDB}
	    MongoDB \cite{Mongodb} is an open-source, document database designed for ease of development and scaling. Classified as a NoSQL 
    database program, MongoDB avoids the traditional table-based relational database structure in favor of JSON-like documents with dynamic 
    schema. It follows a memory hungry approach where it tries to use up most of system and it actually leaves it up to the OS's VMM to tell it 
    to release the memory.

	\myparagraph{Redis}
	    Redis \cite{Redis} is a in-memory data structure store, used as database, cache and message broker. It is used to store a large 
    number of in-memory key-value pairs. Its in-memory nature makes it a prime candidate to use it as a workload in our empirical evaluations.
      
	\myparagraph{YCSB benchmark}
	    We use YCSB \cite{cooper2010benchmarking} (Yahoo Cloud Server Benchmark) project as the benchmark to generate the clients evaluate 
    to the performance of our real workloads i.e MongoDB and Redis servers. The goal of YSCB is to develop performance comparisons of the new 
    generation of cloud data serving systems. It is a framework and common set of workloads for evaluating the performance of different 
    “key-value” stores.
    
  
    \subsection{Issues in native environment}
    
      We have tried to show the issues in container memory management using empirical analysis. We have taken our hypothesis/questions, 
      and mapped them to various experiments to illustrate the issues. Inferences were drawn based on the observations in the experiments. 
    
      \begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{images/controller_issues/native_testbed.png}
	\caption{Native container testbed}
	\label{img:native_setup}
      \end{figure}
      
       The native testbed consisted of running containers inside a host machine (running inside VM in our case) in complete isolation from 
the external environment as shown in Fig~\ref{img:derived_setup}. This setup which involved a native container testbed, was used to 
understand the existing memory reclamations and establish the problem in a native system using \textbf{synthetic workloads}.
	
      \subsubsection{Host}
	
	\begin{enumerate}
	  \item Intel Core i5-4430 processor @ 3.00GHz
	  \item 4 cores of CPU (with hyper threading support)
	  \item 1 TB of hard disk space
	  \item 8 GB RAM
	  \item Ubuntu 14.04 LTS desktop, 64 bit 
	  \item Kernel version 4.5
	  \item KVM Hypervisor
	\end{enumerate}
      
      \subsubsection{Guest}
	
	\begin{enumerate}
	  \item 3 cores of CPU (with hyper threading support)
	  \item 20 GB of virtual disk space
	  \item 2-6 GB RAM (based on experimental configuration)
	  \item Ubuntu 16.04 LTS desktop, 64 bit
	  \item Kernel version 4.7
	  \item Container technology: Docker
	\end{enumerate}
	
      Memory Hogger and File Hogger were used to generate the memory pressure inside the containers. External pressure was generated 
using Stress workload running directly on the host machine.
  
  
      The configuration in Table~\ref{table_native_base} is the base configuration for all experiments in this section. Any changes the base 
      configuration has been mentioned in the procedure of each of the experiment.

	\begin{table}	 
	  \begin{center}
	    \begin{tabular}{ l | c | c }
	      & Container-1 (M1) & Container-2 (M2) \\ 
	      \hline
	      \hline
	      Size of VM & \multicolumn{2}{c}{2 GB} \\	      
	      \hline
	      Workload & Memory Hogger & Memory Hogger \\
	      \hline
	      Hard Limit & 1000 MB & 1000 MB \\  
	      \hline
	      Soft Limit & 150 MB & 150 MB \\  
	      \hline
	      Memory Usage & 500 MB & 500 MB \\
	      \hline
	      Exceed & 350 MB & 350 MB \\
	      \hline 
	      External Pressure & \multicolumn{2}{l}{ 200 - 400 - 600 - 800 - 1000 MB} \\
	    \end{tabular}	    
	    \caption{Base configuration for native container experimentation}
	    \label{table_native_base}
	  \end{center}
	\end{table}
	
      Most experiments involved setting up of 2 containers. Workloads were used to introduce system memory pressure from containers. At 
      this point there was no memory pressure in the system (free memory was still available). Now the external pressure using Stress was 
      introduced after about 20s which created memory pressure in the system that triggered reclamation. The external pressure kept on increasing 
      by 200 MB in intervals of 40s. Each interval had a gap of 10s for memory to be reassigned to containers.
      
      \subsubsection{Reclamation above soft limits}
      
	\myparagraph{Hypothesis} 
	  Hypothesis to be verified,
	  \begin{enumerate}
	    \item Majority of reclamation when containers exceed occurs using SMR (Soft Memory Reclamation) 
	    \item SMR purely based on exceed value of the container
	    \item Containers that exceed equally are iteratively targeted
	  \end{enumerate} 
	  
	  \myparagraph{Procedure}
	    To demonstrate the correctness of our hypothesis we the base configuration described in Table~\ref{table_native_base} and change 
  the usage to 700 MB and soft limit to 350 MB there by simulating an scenario (Exp-1) where \textbf{Both containers exceeded by the same 
  values}.
	  
	  \begin{figure*}[t!]
	    \centering
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/exceed_only/Exceed.png}
	      \caption{Memory exceed plot}
	      \label{img_exceed_only_1_exceed}
	    \end{subfigure}
	    ~ 
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/exceed_only/Memory_Reclaimed.png}
	      \caption{Cumulative soft memory reclaimed plot}
	      \label{img_exceed_only_1_smr}
	    \end{subfigure}
	    ~ 
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/exceed_only/compare.png}
	      \caption{SMR versus GLR}
	    \label{img_exceed_only_1_compare}
	    \end{subfigure}
	    \caption{Plots for analysis of reclamation when both containers are exceeding by same value}
	  \end{figure*}
	  
	  \myparagraph{Observations}
	    The following are the observations,
	    \begin{itemize}
	      \item As seen from Fig~\ref{img_exceed_only_1_exceed}, Fig~\ref{img_exceed_only_1_smr} - memory reclaimed from containers 
  iteratively from one after the other as their exceeds are same.
	      \item Fig~\ref{img_exceed_only_1_compare} shows how most reclamation when containers exceed occurs using SMR however it is seen 
  that there is minimum reclamation occurring using GLR as well.
	    \end{itemize}

	  \myparagraph{Inference}	
	    The following are the inferences,
	    \begin{itemize}
	      \item SMR is purely based on exceed value.
	      \item Most reclamation when containers exceed occurs using SMR, however the GLR kicks in every reclamation request to evict any 
  inactive page cache pages in the system (may/may not belong to container).
	      \item Containers that exceed equally are iteratively target for reclamation one after the other.
	    \end{itemize}
      
      \subsubsection{Reclamation below soft limits}
      
      \myparagraph{Hypothesis}
	Does our hypotheses of reclamation below soft limits falling back to native system reclamation hold good ?
	
      \myparagraph{Procedure}  
	To test the reclamation patterns in containers below soft limits, we created containers as mentioned in Table~\ref{table_native_base} 
  and changed soft limits (Exp-2) of both containers to 1000 MB there by making the current \textbf{usage of both containers below soft 
  limits}. We used hooks in the kernel code to track requests satisfied by soft memory reclamation (SMR) and global LRU based reclamation 
  (GLR).
	
	\begin{figure*}[t!]
	    \centering
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/global_lru/mu.png}
	      \caption{Memory Usage Plot}
	      \label{img_no_sl_mu}
	    \end{subfigure}
	    ~ 
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/global_lru/compare.png}
	      \caption{SMR versus GLR Plot}
	      \label{img_no_sl_global_vs_local}
	    \end{subfigure}
	    \caption{Plots for when both containers are having same usage but no exceeds}
	  \end{figure*}
	
	\myparagraph{Observations}
	  The following are the observations.
	  \begin{itemize}
	    \item As seen from Fig~\ref{img_no_sl_mu}, there is no hand-in-hand reclamation that occurs to containers below their soft 
  limits although the containers are running the same workload, unlike hand in hand reclamation that occurs in memory usage above soft limits.
	    \item Since both containers are below SL, all reclamation is occurring using the GLR (Global LRU based reclamation) as seen by 
  Fig~\ref{img_no_sl_global_vs_local}
	  \end{itemize}

	\myparagraph{Inferences}
	  The following are the inferences.
	  \begin{itemize}
	    \item Containers with memory usage below soft limits reclamation falls back to native system GLR.
	    \item Reclamation using GLR is haphazard and there is no control over it. 
	  \end{itemize}
   
     \subsubsection{Effect of workloads characteristics on reclamation}
      
	\myparagraph{Question}
	Questions of our interest,
	    \begin{enumerate}
	      \item Effect of workload characteristics on reclamation
	      \item How much of memory is reclaimed from a container in a single reclamation SMR request ?
	    \end{enumerate}
	  
	  \myparagraph{Procedure}
	    We took our base configuration as described in Table~\ref{table_native_base}. However we ran two workloads in this case - Memory 
  Hogger (Exp-4a) and File Hogger (Exp-4b) workloads on it as native theory suggests that containers with page cache pages might be 
  victimized at larger the way it occurs with GLR.	
	  

	  \begin{figure*}[t!]
	    \centering
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/workload/1/Exceed.png}
	      \caption{Exceed plot for Experiment-4a}
	      \label{img:workload_1_exceed}
	    \end{subfigure}
	    ~ 
	    \begin{subfigure}[t]{0.48\textwidth}
	      \centering
	      \includegraphics[width=0.8\textwidth]{images/controller_issues/workload/2/Exceed.png}
	      \caption{Exceed plot for Experiment-4b}
	      \label{img:workload_2_exceed}
	    \end{subfigure}
	    \caption{Plots for analyzing effect of workloads characteristics on reclamation}
	  \end{figure*}		
	    
	  \myparagraph{Observations}
	  The following were the observations,	  
	  \begin{itemize}
	    \item The exceed goes hand in hand as expected but with larger deviation in Fig~\ref{img:workload_1_exceed} and 
  Fig~\ref{img:workload_2_exceed}
	    \item The larger deviation can be accounted to larger reclamation chunks in workloads that have page cache pages similar to how 
  reclamation targets page cache pages in native system
	    \item Further empirical analysis of the reclamation chucks gave us the reclamation chucks to be 
		\begin{center}
		    Reclamation chuck = Anonymous memory pages (\textless 25MB) + Page cache pages
		\end{center}
		In both cases pages from inactive zones were reclaimed before trying to reclaim from active lists.
	  \end{itemize}

	  \myparagraph{Inference}
	    Workloads with page cache pages are reclaimed at larger chunks per SMR request
	  
      \subsubsection{Key Implications}
    
	Here are the list of key implications that were derivative from running the above experiments in an synthetic environment. We have 
    classified it based on the scenarios as discussed earlier.
	
	\begin{enumerate}
	  \item When containers usage are above soft limits most reclamation occurs using SMR, however the GLR kicks in every reclamation 
    request to evict any  inactive page cache pages in the system (may/may not belong to container).      
	  \item SMR is purely based on exceed value of a container.
	  \item Workloads with page cache pages are reclaimed at larger chunks per SMR request
	  \item Containers with memory usage below soft limits reclamation falls back to native system GLR.
	  \item Reclamation using GLR is haphazard and there is no control over it. 
	\end{enumerate}
	
  
    \subsection{Amplification of issue in derivative clouds}
    
      The following experiment tries to establish the implications of previously established inferences, as to how these affect 
applications running on a derived cloud environment.

      The derivative cloud testbed consisted of running server containers inside a virtual machine (VM-1) which was running on top of a 
physical host machine. Another virtual machine (VM-2) was used to generate clients who connected to servers containers running inside VM-1 
as shown in Fig~\ref{img:derived_setup}. This setup was used to understand the impact of existing memory reclamation patterns on real 
workloads running on a derivative cloud setting.
      
      \begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{images/controller_issues/derivative_setup.png}
	\caption{Derivative cloud testbed}
	\label{img:derived_setup}
      \end{figure}
      
      \paragraph{Host}	
	\begin{enumerate}
	  \item Intel Xeon E5507 @ 2.27GHz
	  \item 8 cores of CPU (with hyper-threading support)
	  \item 125 GB of attached storage, Unlimited NFS attached storage 
	  \item 24 GB RAM
	  \item Ubuntu 14.04 LTS server, 64 bit 
	  \item Kernel version 3.13
	  \item KVM Hypervisor with memory ballooning enabled
	  \item Guest machines were connected using a software bridge
	\end{enumerate}
      
      \myparagraph{Guest}
	The two VMs used in this setup are described here.
	
	\noindent \textbf{VM-1:} Running server containers
	\begin{enumerate}
	  \item 6 cores of pinned CPUs (with hyper threading support)
	  \item 175 GB of virtual disk space (Storage was provisioned using NFS)
	  \item 16 GB RAM
	  \item Ubuntu 16.04 LTS desktop, 64 bit
	  \item Kernel version 4.7
	  \item Container technology: Docker
	  \item Containers inside guest were multiplexed using NAT forwarding
	\end{enumerate}
	
	\noindent \textbf{VM-2:} Running clients that connect to server containers
	\begin{enumerate}
	  \item 1 core of pinned CPU (with hyper threading support)
	  \item 20 GB of virtual disk space (Storage was provisioned using NFS)
	  \item 6 GB RAM
	  \item Ubuntu 16.04 LTS desktop, 64 bit
	  \item Kernel version 4.7
	\end{enumerate}
	
	Redis and MongoDB was used to generate the memory pressure inside the containers. External pressure was generated by varying guest 
balloon size triggered from the host.

      
    \subsubsection{Experimental Flow}
      
      In the experimental setup, 4 containers configured as discussed above. Containers contained server 
    workloads (Redis/MongoDB) to which clients had to connect. There were two types of containers for each workload i.e Redis and MongoDB. One 
    of each with low priority and other of each with higher priority. 

    The relative avg. usage of the low : higher priority containers are in the ratio of 1:2 and so are their provisioning. Such provisioning 
    makes us \textbf{expect as 1:1 throughput} in terms of application performance between the low and high priority workloads in each case in 
    an ideal scenario. The default configurations for the four containers are given on Table~\ref{table:default_config}.

	\begin{table}[!htb]
	  \begin{center}	   
	    \begin{tabular}{ l | p{2cm} | p{2cm} | p{4cm} | p{2cm} }
	      Container & HL (GB) & SL (GB) & Workload Size (records) & Avg. Usage (GB) \\ 
	      \hline
	      \hline
	      Redis-Low & 2 & 0.5 & 500K & 1.3 \\  
	      \hline
	      Mongo-Low & 2 & 0.5 & 500K & 1.3 \\
	      \hline
	      Redis-High & 4 & 1 & 1000K & 2.6 \\  
	      \hline
	      Mongo-High & 4 & 1 & 1000K & 2.6
	    \end{tabular}	  
	  \end{center}
	  \caption{Base configuration for derived cloud experimentation}
	  \label{table:default_config}	  
	\end{table}

	Containers were created and datasets were loaded off-line to generated the initial memory pressure required. Each container 
    had only two clients attached to each of them to avoid CPU bounded contention. The clients were setup on VM-2. All containers were 
    over provisioned for all resources other than memory.     
	
    	
	Initially 100s was given to allow the containers to setup and consume desired memory without any external pressure. Once they 
    settled, pressure was constantly generated at a rate of x GB every 30s from the host using a balloon driver to constantly reduce the 
    memory available to VM-1. Experiments were carried out by varying soft limits, workload size (usage) and external pressure (generated by 
    balloon driver).
    
     \subsubsection{Impact in derivative environment}
	We demonstrate the impact of existing memory reclamation on containers in a derivative setup and how it affects its application performance.
    
      \begin{figure*}[t!]
	  \centering
	  \begin{subfigure}[t]{0.48\textwidth}
	    \centering
	    \includegraphics[width=1\textwidth]{images/controller_issues/derivative/complete.png}
	    \caption{Exceed plot}
	    \label{plot_inference_complete}
	  \end{subfigure}
	  ~ 
	  \begin{subfigure}[t]{0.48\textwidth}
	    \centering
	    \includegraphics[width=1\textwidth]{images/controller_issues/derivative/complete_mongo.png}
	    \caption{Throughput of MongoDB containers}
	    \label{plot_inference_complete_mongo}
	  \end{subfigure}
	  ~ 
	  \begin{subfigure}[t]{0.48\textwidth}
	    \centering
	    \includegraphics[width=1\textwidth]{images/controller_issues/derivative/complete_redis.png}
	    \caption{Throughput of Redis containers}
	    \label{plot_inference_complete_redis}
	  \end{subfigure}
	  \caption{Plots for analysis of complete (above \& below SL) reclamation in derivative setup}
	\end{figure*}
	
	\subsubsection{Question:}
	  \begin{enumerate}	    
	    \item How does the entire process of reclamation after throughput of containers moving from all containers exceeding 
	    their soft limits to none of them exceeding in a derivative environment ?
	  \end{enumerate}	
	
	\subsubsection{Procedure:}
	  We begin with containers configured as described in Table~\ref{table:default_config}. The balloon driver is inflated inside the 
guest to change memory inside VM in steps of 16-14-12-10-9-8-7-6-5-4-3 GB every 30s. 
	
	\subsubsection{Observations:}
	  \begin{enumerate}
	    \item Fig~\ref{plot_inference_complete_mongo}, Fig~\ref{plot_inference_complete_redis} shows how application throughput of 
MongoDB and Redis is impacted negatively using the existing knobs when provisioning for deterministic allocation as observed in between 
t=100 and t=200.
	    \item When reclamation goes agnostic of existing knobs (after t=200), it appears to have more desired throughputs but this 
maybe accounted for the nature of the current workloads that actively consume memory. 
	    \item  In case of a workload that uses memory after certain intervals, this situation may achieve undesired throughputs even 
below soft limits. This needs further experimentation.
	    \item As observed here container soft limits are violated when the system is under immense pressure or the container soft 
limits are over provisioned. However, the system tries its best efforts to maintain soft limits.  
	  \end{enumerate}
	  
	\subsubsection{Inferences:}
	  \begin{enumerate}
	    \item The container aware reclamation above soft limits impact negatively in when trying to achieve deterministic provisioning 
with QOS guarantees.
	    \item There is a lot of non determinism in reclamations below soft limits in the current policy.
	    \item Soft limits are not guarantees, but are mere best effort approach.
	  \end{enumerate}

  \subsection{Key sights and drawbacks of existing system}
    \begin{enumerate}
      \item Reclamation above SL is based on exceed values of each container, may impact negatively while we try to provision containers 
based on QOS guarantees.
      \item Reclamation below SL falls back to host LRU based reclamation without taking into container provisioning and this leads to a 
lot of non-determinism.
      \item Soft Limit is not a definite guarantee, it is mere best effort approach.
    \end{enumerate}
    
    
  \section{Requirements for a new memory management controller}
  
    We wish to build an updated memory management controller that is controller aware, and is able to enforce a differential management
    policy by a native or derivative cloud provider. The following are the list of requirements of policies that we would like to enforce
    using this controller,
  
    \begin{enumerate}
      \item \textbf{Prioritized memory allocation:} Currently the notion of priority doesn't exist in container specific memory allocation 
although the notion of priority exists in other resources. The existing knobs fail to enforce priority used to manage memory in containers. 
      \item \textbf{Deterministic provisioning:} The policy to be designed must eliminate existing non determinism that exists while 
managing memory between containers in existing system.
      \item \textbf{Elastic provisioning:} Memory allocated must be resizable as and when required. 
      \item \textbf{Adaptive:} On changing resources provisioned to the system as in the case of an derivative environment, the policy 
enforced must still do it's best in maintaining promised QOS.
      \item \textbf{Differentiated memory reclamation:} The policy could build around the notion of differentiated memory reclamation when 
the system falls under memory pressure.
      \item \textbf{Strict enforcement of limits:} The notion of hard and soft limits that exist must be strengthened.      
    \end{enumerate}
    
  We would like to design a new controller keeping the above requirements in mind.
   
    
  
  \section{Proposed memory management controller}
    
      \subsection{Controller architecture}
    
      \subsection{Policies supported by the controller}
	
  
  
  \section{Modifications made to Linux memory Cgroup}
  
    \subsection{Per container configurable weights}
    
    \subsection{Deterministic reclamation}
    
    \subsection{Flexible reclamation size}
    
  
  
  \section{Empirical evaluation of our controller}
  
    \subsection{Effectiveness of our controller}
    
    \subsection{Differential QOS containers}
    
    \subsection{Impact of reclamation chunk size}